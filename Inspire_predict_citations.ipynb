{
 "metadata": {
  "name": "",
  "signature": "sha256:3ac545560e47e310507978dee4e3c384e0fef65e585b802295496df826043eac"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.display import HTML\n",
      "\n",
      "HTML('''<script>\n",
      "code_show=true; \n",
      "function code_toggle() {\n",
      " if (code_show){\n",
      " $('div.input').hide();\n",
      " } else {\n",
      " $('div.input').show();\n",
      " }\n",
      " code_show = !code_show\n",
      "} \n",
      "$( document ).ready(code_toggle);\n",
      "</script>\n",
      "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>''')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<script>\n",
        "code_show=true; \n",
        "function code_toggle() {\n",
        " if (code_show){\n",
        " $('div.input').hide();\n",
        " } else {\n",
        " $('div.input').show();\n",
        " }\n",
        " code_show = !code_show\n",
        "} \n",
        "$( document ).ready(code_toggle);\n",
        "</script>\n",
        "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 1,
       "text": [
        "<IPython.core.display.HTML at 0x3978390>"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "import matplotlib\n",
      "matplotlib.style.use('ggplot')\n",
      "%matplotlib inline\n",
      "\n",
      "import read_Inspire_data\n",
      "from path import PATH\n",
      "\n",
      "from sklearn.cross_validation import KFold\n",
      "from sklearn import cross_validation\n",
      "\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.svm import SVR"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#PATH = \"C:/Users/Ola/Documents/Inspire_data_project/\"\n",
      "#PATH = \"C:/Users/Oluszka/Documents/inspire_data/\"\n",
      "FILE_NAME = \"hep_records.json\"\n",
      "df_clean = read_Inspire_data.load_inspire(PATH+FILE_NAME)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "start_year = 1990 #1910\n",
      "actual_year = 2010 #2015\n",
      "raw_data = df_clean[(df_clean['year'] >= start_year) & (df_clean['year'] <= actual_year)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Citation count is one of the most important factors when deciding the impact of a scientist on physics and physics community. As such, it is under constant research. I am interested in whether it is possible to predict citation count of  given paper with machine learning methods and InspireHEP database. We will concentrate on predicting citation count after 5 years (60 months)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Nmonths = 60\n",
      "Nyears = Nmonths/12"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's start our analysis from the keywords assigned to each paper in the database. There is ~200 thousands standardized keywords. However performing simple operations like using only lower case characters and removing punctuation substantially (by ~20%) reduces the number of different keywords ."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Some small analysis of the keywords:\n",
      "import string\n",
      "exclude = set(string.punctuation)\n",
      "\n",
      "SKeyWords = (list(raw_data.standardized_keywords))\n",
      "#flatten keywords list\n",
      "SKeyWords_flat = [keyword for keywordlist in SKeyWords for keyword in keywordlist] \n",
      "#lowercase, strip and remove punctuation\n",
      "SKeyWords_norm = [ ''.join(ch for ch in keyword.lower() if ch not in exclude).strip() for keyword in SKeyWords_flat] \n",
      "SKeyWords_unique = set(SKeyWords_norm)\n",
      "#print '# of all keywords:', len(SKeyWords_flat)\n",
      "print '# of all unique keywords:', len(set(SKeyWords_flat))\n",
      "print '# of all unique keywords after normalization:', len(SKeyWords_unique)\n",
      "print 'max number of keywords per paper:', raw_data.std_kwd_count.max()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "# of all unique keywords: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "121820\n",
        "# of all unique keywords after normalization: 116242\n",
        "max number of keywords per paper: 65\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import collections\n",
      "counter = collections.Counter(SKeyWords_norm)\n",
      "best_kwrds = [x[0] for x in counter.most_common(32) if x[0] != 'automatic keywords' or x[0] != 'bibliography'] #+ ['review', 'conference']\n",
      "print 'Most common keywords:'\n",
      "print counter.most_common(32)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Most common keywords:\n",
        "[(u'numerical calculations', 71711), (u'bibliography', 23160), (u'numerical calculations interpretation of experiments', 23086), (u'experimental results', 22342), (u'supersymmetry', 21679), (u'electron positron annihilation', 15734), (u'numerical calculations monte carlo', 15512), (u'quantum chromodynamics', 15050), (u'talk', 15013), (u'thesis', 14340), (u'string model', 13117), (u'field theory scalar', 13030), (u'dimension 2', 12165), (u'critical phenomena', 10819), (u'cern lhc coll', 10131), (u'cp violation', 9709), (u'perturbation theory higherorder', 9210), (u'scattering heavy ion', 8489), (u'lattice field theory', 8358), (u'field theory conformal', 7982), (u'electron positron colliding beams', 7702), (u'dimension 3', 7538), (u'gauge field theory yangmills', 7337), (u'hamiltonian formalism', 7323), (u'neutrino oscillation', 7111), (u'effective lagrangian', 6940), (u'field equations solution', 6922), (u'effective action', 6699), (u'duality', 6695), (u'supergravity', 6680), (u'cosmological model', 6629), (u'gravitation', 6612)]\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We have excluded 'bibliography' and 'automatic keywords' from the list of most common keywords and created a list of 30 best keywords for further analysis. We will use them later on when preparing the final feature engineering on our data."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As another interesting feature we will include average citation count of references included in the papers bibliography."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#as a first stepcreate a dictionary with date as values for each key: recid\n",
      "\n",
      "import os.path\n",
      "import csv\n",
      "\n",
      "FILE_DICT_date = \"recid_date.csv\"\n",
      "if os.path.isfile(PATH+FILE_DICT_date):\n",
      "    recid_date = {}\n",
      "   \n",
      "    reader = csv.reader(open(PATH_RECID_DICT, 'rb'))\n",
      "    recid_date = dict(x for x in reader)\n",
      "\n",
      "else:\n",
      "    recid_date = {}\n",
      "    for row in  df_clean.iterrows():\n",
      "        row_num, actual_row = row\n",
      "        recid_date[actual_row['recid']] = actual_row['date']\n",
      "  \n",
      "    with open(PATH+FILE_DICT_date, \"wb\") as outfile:\n",
      "        writer = csv.writer(outfile)\n",
      "        for key, value in recid_date.items():\n",
      "            writer.writerow([key, value])\n",
      "            \n",
      "print len(recid_date.keys())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1082646\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# we create a dictionary with citation counts after 5 years as values for each key:\n",
      "# recid of a paper in the data frame. We save this dictionary to a file.\n",
      "# we use ALL data to create this dictionary (df_clean instead of raw_data).\n",
      "\n",
      "import os.path\n",
      "import csv\n",
      "\n",
      "FILE_RECID_DICT = \"recid_citcount_5.csv\"\n",
      "if os.path.isfile(PATH+FILE_DICT):\n",
      "    recid_citation_5 = {}\n",
      "   \n",
      "    reader = csv.reader(open(PATH+FILE_RECID_DICT, 'rb'))\n",
      "    recid_citation_5 = dict(x for x in reader)\n",
      "\n",
      "else:\n",
      "    recid_citation_5 = {}\n",
      "    for row in  df_clean.iterrows():\n",
      "        row_num, actual_row = row\n",
      "        paper_date = actual_row['date']\n",
      "        recid_citation_5[actual_row['recid']] = 0\n",
      "        for cpaper in actual_row['citations']:\n",
      "            if cpaper in recid_date:\n",
      "                citation_date = recid_date[cpaper]\n",
      "                if 0 < (citation_date - paper_date).days/30 < Nmonths:\n",
      "                    recid_citation_5[actual_row['recid']] += 1\n",
      "  \n",
      "    with open(PATH+FILE_RECID_DICT, \"wb\") as outfile:\n",
      "        writer = csv.writer(outfile)\n",
      "        for key, value in recid_citation_5.items():\n",
      "            writer.writerow([key, value])\n",
      "            \n",
      "print len(recid_citation_5.keys())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1082646\n"
       ]
      }
     ],
     "prompt_number": 53
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# we create a dictionary with citation counts as values for each key:\n",
      "# recid of a paper in the data frame. We save this dictionary to a file.\n",
      "# we use ALL data to create this dictionary (df_clean instead of raw_data).\n",
      "\n",
      "import os.path\n",
      "import csv\n",
      "\n",
      "FILE_DICT = \"recid_citcount_tot.csv\"\n",
      "if os.path.isfile(PATH+FILE_DICT):\n",
      "    recid_citation_tot = {}\n",
      "   \n",
      "    reader = csv.reader(open(PATH_RECID_DICT, 'rb'))\n",
      "    recid_citation_tot = dict(x for x in reader)\n",
      "\n",
      "else:\n",
      "    recid_citation_tot = {}\n",
      "    for row in  df_clean.iterrows():\n",
      "        row_num, actual_row = row\n",
      "        recid_citation_tot[actual_row['recid']] = actual_row['cit_count']\n",
      "  \n",
      "    with open(PATH + FILE_DICT, \"wb\") as outfile:\n",
      "        writer = csv.writer(outfile)\n",
      "        for key, value in recid_citation_tot.items():\n",
      "            writer.writerow([key, value])\n",
      "            \n",
      "print len(recid_citation_tot.keys())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1082646\n"
       ]
      }
     ],
     "prompt_number": 54
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Now we prepare the function that will take average citation count\n",
      "# Watch out - not all recid's might be in the cleandata database. \n",
      "# We only include 'clean' references!\n",
      "\n",
      "def average_cit_count(recid_list):\n",
      "    citations = [recid_citation_tot[elem] for elem in recid_list if elem in recid_citation_tot]\n",
      "    if len(citations)==0:\n",
      "        return 0\n",
      "    return float(sum(citations))/len(citations)\n",
      "\n",
      "print 'sample average cit count: ', average_cit_count(raw_data['citations'][2603])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "sample average cit count:  24.5531914894\n"
       ]
      }
     ],
     "prompt_number": 55
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_clean['recid'][1345]\n",
      "recid_citation_tot[1370]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 60,
       "text": [
        "18L"
       ]
      }
     ],
     "prompt_number": 60
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We will try to predict wether the paper falls into one of the following Inspire categories for citation counts: \n",
      "Renowned papers (500+ citations)\t\n",
      "Famous papers (250-499 citations)\t\n",
      "Very well-known papers (100-249 citations)\t\n",
      "Well-known papers (50-99 citations)\t\n",
      "Known papers (10-49 citations)\t\n",
      "Less known papers (1-9 citations)\t\n",
      "Unknown papers (0 citations)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def inspire_citation_category(x):\n",
      "    if x >= 500:\n",
      "        return 'Renowned'\n",
      "    if x >= 250:\n",
      "        return  'Famous'\n",
      "    if x >= 100:\n",
      "        return  'Very-well-known'\n",
      "    if x >= 50:\n",
      "        return  'Well-known'\n",
      "    if x >= 10:\n",
      "        return  'Known'\n",
      "    if x >= 1:\n",
      "        return  'Less-known'\n",
      "    return  'Unknown'\n",
      "\n",
      "print 'Categorization:'\n",
      "print 'with 0 citations:', inspire_citation_category(0)\n",
      "print 'with 3 citations:', inspire_citation_category(1)\n",
      "print 'with 50 citations:', inspire_citation_category(50)\n",
      "print 'with 120 citations:', inspire_citation_category(100)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Categorization:\n",
        "with 0 citations: Unknown\n",
        "with 3 citations: Less-known\n",
        "with 50 citations: Well-known\n",
        "with 120 citations: Very-well-known\n"
       ]
      }
     ],
     "prompt_number": 56
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#\n",
      "# feature engineering \n",
      "#\n",
      "\n",
      "def enhance_data(raw_data, shuffle=True):\n",
      "    # 80% data for a training set, 20% for a test set\n",
      "    if shuffle:\n",
      "        data = raw_data.reindex(np.random.permutation(raw_data.index))\n",
      "    else:\n",
      "        data = raw_data.copy()\n",
      "    \n",
      "    # adding a normalized column with standarized keywords to the dataframe\n",
      "    data['std_kwd'] = map(lambda klist: [''.join(ch for ch in keyword.lower() if ch not in exclude) for keyword in klist], data['standardized_keywords'])\n",
      "    \n",
      "    #adding most popular kwrds as features\n",
      "    for elem in best_kwrds:\n",
      "        data[elem] = elem in data['std_kwd']\n",
      "\n",
      "    #adding average citation count of references\n",
      "    data['av_cit_of_references'] = map(average_cit_count, data['citations'])    \n",
      "    \n",
      "    #adding citation count after 5 years \n",
      "    data['cit_count_5'] = map(lambda x: int(recid_citation_5[x]) if x in recid_citation_5 else 0, data['recid'])\n",
      "    \n",
      "    #adding citation category as outcome \n",
      "    data['cit_count_cat'] = map(lambda x: inspire_citation_category(float(x)), data['cit_count'])\n",
      "    \n",
      "    #adding 5-year-citation category as outcome \n",
      "    data['cit_count_cat_5'] = map(lambda x: inspire_citation_category(float(x)), data['cit_count_5'])\n",
      "        \n",
      "    return data\n",
      "\n",
      "data = enhance_data(raw_data)\n",
      "print data.columns"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Index([u'abstract', u'citations', u'free_keywords', u'recid', u'references', u'standardized_keywords', u'title', u'tot_auth_count', u'tot_auth_list', u'cit_count', u'free_kwd_count', u'std_kwd_count', u'ref_count', u'title_len', u'abstract_len', u'date', u'year', u'std_kwd', u'numerical calculations', u'bibliography', u'numerical calculations interpretation of experiments', u'experimental results', u'supersymmetry', u'electron positron annihilation', u'numerical calculations monte carlo', u'quantum chromodynamics', u'talk', u'thesis', u'string model', u'field theory scalar', u'dimension 2', u'critical phenomena', u'cern lhc coll', u'cp violation', u'perturbation theory higherorder', u'scattering heavy ion', u'lattice field theory', u'field theory conformal', u'electron positron colliding beams', u'dimension 3', u'gauge field theory yangmills', u'hamiltonian formalism', u'neutrino oscillation', u'effective lagrangian', u'field equations solution', u'effective action', u'duality', u'supergravity', u'cosmological model', u'gravitation', u'av_cit_of_references', u'cit_count_5', u'cit_count_cat', u'cit_count_cat_5'], dtype='object')\n"
       ]
      }
     ],
     "prompt_number": 62
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# choose keys for machine learning\n",
      "n = len(data)\n",
      "feats = ['year', 'tot_auth_count', 'ref_count','title_len', 'abstract_len', 'av_cit_of_references'] + best_kwrds\n",
      "#print feats\n",
      "\n",
      "X = np.array(data.get(feats))\n",
      "y = np.array(data.cit_count_cat)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 66
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# cross validation: dividing data set into train and test data\n",
      "\n",
      "# function to evaluate the model:\n",
      "def score(clf):\n",
      "    # use k-fold to divide train and validation\n",
      "    kf = KFold(n, n_folds=5)\n",
      "    # will get 5 scores\n",
      "    scores = []\n",
      "    for train, test in kf:\n",
      "        clf.fit(X[train], y[train])\n",
      "        # use sclearn score:\n",
      "        scores.append(clf.score(X[test], y[test]))\n",
      "    return sum(scores)/len(scores)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 67
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print score(RandomForestClassifier())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.70657405553\n"
       ]
      }
     ],
     "prompt_number": 68
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# choose keys for machine learning\n",
      "n = len(data)\n",
      "feats = ['year', 'tot_auth_count', 'ref_count','title_len', 'abstract_len', 'av_cit_of_references'] + best_kwrds\n",
      "#print feats\n",
      "\n",
      "X_train2, X_test2, y_train2, y_test2 = cross_validation.train_test_split(data.get(feats), data.cit_count, test_size=0.4, random_state=0)\n",
      "\n",
      "#X2 = np.array(data.get(feats))\n",
      "#y2 = np.array(data.cit_count)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model_regr = LinearRegression()\n",
      "model_regr.fit(X_train2,y_train2)\n",
      "print model_regr.score(X_test2, y_test2)     "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.0856603788114\n"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model_regr = SVR()\n",
      "model_regr.fit(X_train2,y_train2)\n",
      "print model_regr.score(X_test2, y_test2)    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}