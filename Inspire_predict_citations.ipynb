{
 "metadata": {
  "name": "",
  "signature": "sha256:aa0fae289e83e0128b74d4e55d67b5bea952de1a0eec6209a4866dee4daf8473"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import json\n",
      "\n",
      "import matplotlib\n",
      "matplotlib.style.use('ggplot')\n",
      "%matplotlib inline\n",
      "\n",
      "import read_Inspire_data\n",
      "from data_path import DATA_PATH, DATA_FILE_NAME, FILE_NAMES, FILE_AUTH, FILE_DICT_recid\n",
      "\n",
      "from sklearn.cross_validation import KFold\n",
      "from sklearn import cross_validation\n",
      "\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.svm import SVR"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 72
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_clean = read_Inspire_data.load_inspire(DATA_PATH+DATA_FILE_NAME)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Citation count is one of the most important factors when deciding the impact of a scientist on physics and physics community. As such, it is under constant research. I am interested in whether it is possible to predict citation count of  given paper with machine learning methods and InspireHEP database. We will concentrate on predicting citation count after 5 years (60 months) from the date of publication of a paper. We will concentrate on papers published between 1980 and 2009 (later papers might not have a proper 5 year citation count). We filter our data accordingly."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The InspireHEP database with papers, references, citations and authors is an extremely complex data source. Adding a new paper into Inspire, influences statistics of many other papers, by boosting their citations counts. When performing machine learning, we must be aware of those influences. We are doing a many simplifying assumptions, to make the predictions work. Our goal is, to estimate the 5-year citation count of a new paper, that is ready to be added to the database, based on the data we currently have, and common features of the paper of interest: its title, abstract, keywords, authors, bibliography. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# filter data based on year of publication\n",
      "start_year = 1985\n",
      "actual_year = 2008\n",
      "raw_data = df_clean[(df_clean['year'] >= start_year) & (df_clean['year'] <= actual_year)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 77
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#To work with 5 year citation threshold we define the threshhold for years and months.\n",
      "Nmonths = 60\n",
      "Nyears = Nmonths/12"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 78
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "***"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's start our analysis from studying the keywords assigned to each paper in the database. There is ~200 thousands standardized keywords in the database total. However performing simple operations like using only lower case characters and removing punctuation substantially (by ~20%) reduces the number of different keywords. For the filtered data in particular we have:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Some small analysis of the keywords:\n",
      "import string\n",
      "exclude = set(string.punctuation)\n",
      "\n",
      "SKeyWords = (list(raw_data.standardized_keywords))\n",
      "\n",
      "#flatten keywords list\n",
      "SKeyWords_flat = [keyword for keywordlist in SKeyWords for keyword in keywordlist] \n",
      "\n",
      "#lowercase, strip and remove punctuation\n",
      "SKeyWords_norm = [ ''.join(ch for ch in keyword.lower() if ch not in exclude).strip() for keyword in SKeyWords_flat] \n",
      "SKeyWords_unique = set(SKeyWords_norm)\n",
      "\n",
      "#print '# of all keywords:', len(SKeyWords_flat)\n",
      "print '# of all unique keywords:', len(set(SKeyWords_flat))\n",
      "print '# of all unique keywords after normalization:', len(SKeyWords_unique)\n",
      "print 'max number of keywords per paper:', raw_data.std_kwd_count.max()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "# of all unique keywords: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "155769\n",
        "# of all unique keywords after normalization: 126535\n",
        "max number of keywords per paper: 65\n"
       ]
      }
     ],
     "prompt_number": 79
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import collections\n",
      "counter = collections.Counter(SKeyWords_norm)\n",
      "best_kwrds = [x[0] for x in counter.most_common(32) if x[0] != 'automatic keywords' or x[0] != 'bibliography'] #+ ['review', 'conference']\n",
      "print 'Most common keywords:'\n",
      "print counter.most_common(32)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Most common keywords:\n",
        "[(u'numerical calculations', 74543), (u'talk', 33653), (u'numerical calculations interpretation of experiments', 28034), (u'experimental results', 26243), (u'bibliography', 26091), (u'supersymmetry', 24862), (u'electron positron annihilation', 18196), (u'numerical calculations monte carlo', 17109), (u'quantum chromodynamics', 15477), (u'thesis', 15247), (u'field theory scalar', 13877), (u'string model', 13342), (u'dimension 2', 12210), (u'perturbation theory higherorder', 10595), (u'cp violation', 10516), (u'critical phenomena', 10318), (u'scattering heavy ion', 9972), (u'electron positron colliding beams', 9280), (u'lattice field theory', 8848), (u'gauge field theory yangmills', 8831), (u'field theory conformal', 7926), (u'field equations solution', 7813), (u'cern lhc coll', 7753), (u'annihilation electron positron', 7579), (u'gravitation', 7468), (u'neutrino oscillation', 7348), (u'supergravity', 7335), (u'hamiltonian formalism', 7129), (u'dimension 3', 6983), (u'cosmological model', 6862), (u'feynman graph', 6793), (u'effective lagrangian', 6702)]\n"
       ]
      }
     ],
     "prompt_number": 80
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We have excluded 'bibliography' and 'automatic keywords' from the list of most common keywords and created a list of 30 best keywords for further analysis. We will use them later on when preparing the final feature engineering on our data."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "***"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A very interesting feature could be an average citation count of references included in the papers bibliography. However, adding this feature makes it more difficult to validate the model. A random k-fold cross validation is not reliable anymore. We will try to deal with this issue later and modify citation count, for now let's see how to add a simple citation count. We we'll get help from a previously created dictionary containing recid and citing papers, prepared with make_dicts.py."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "recid_citation= {}\n",
      "with open(DATA_PATH + FILE_DICT_recid) as infile:\n",
      "    for line in infile:\n",
      "        key, value = json.loads(line)\n",
      "        recid_citation[key] = value     "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Now we prepare the function that will take average citation count\n",
      "# We only inlcude citations before a given year, for a future validation of our predictions. \n",
      "# Watch out - not all recid's might be in the cleandata database. \n",
      "# We only include 'clean' references!\n",
      "\n",
      "def average_cit_count(recid_list, year):\n",
      "    citations = [recid_citation[elem]['cit_tot'] for elem in recid_list \n",
      "                 if elem in recid_citation]\n",
      "    if len(citations)==0:\n",
      "        return 0\n",
      "    return float(sum(citations))/len(citations)\n",
      "\n",
      "print 'sample average cit count for recid 2603: ', average_cit_count(raw_data['citations'][2603], 2004)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "sample average cit count for recid 2603:  24.7234042553\n"
       ]
      }
     ],
     "prompt_number": 86
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "***"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We will try to predict wether the paper falls into one of the following Inspire categories for citation counts: \n",
      "Renowned papers (500+ citations)\t\n",
      "Famous papers (250-499 citations)\t\n",
      "Very well-known papers (100-249 citations)\t\n",
      "Well-known papers (50-99 citations)\t\n",
      "Known papers (10-49 citations)\t\n",
      "Less known papers (1-9 citations)\t\n",
      "Unknown papers (0 citations)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def inspire_citation_category(x):\n",
      "    if x >= 500:\n",
      "        return 'Renowned'\n",
      "    if x >= 250:\n",
      "        return  'Famous'\n",
      "    if x >= 100:\n",
      "        return  'Very-well-known'\n",
      "    if x >= 50:\n",
      "        return  'Well-known'\n",
      "    if x >= 10:\n",
      "        return  'Known'\n",
      "    if x >= 1:\n",
      "        return  'Less-known'\n",
      "    return  'Unknown'\n",
      "\n",
      "print 'Categorization:'\n",
      "print 'with 0 citations:', inspire_citation_category(0)\n",
      "print 'with 3 citations:', inspire_citation_category(1)\n",
      "print 'with 50 citations:', inspire_citation_category(50)\n",
      "print 'with 120 citations:', inspire_citation_category(100)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Categorization:\n",
        "with 0 citations: Unknown\n",
        "with 3 citations: Less-known\n",
        "with 50 citations: Well-known\n",
        "with 120 citations: Very-well-known\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#\n",
      "# feature engineering \n",
      "#\n",
      "\n",
      "def enhance_data(raw_data, shuffle=True):\n",
      "    # 80% data for a training set, 20% for a test set\n",
      "    if shuffle:\n",
      "        data = raw_data.reindex(np.random.permutation(raw_data.index))\n",
      "    else:\n",
      "        data = raw_data.copy()\n",
      "    \n",
      "    # adding a normalized column with standarized keywords to the dataframe\n",
      "    data['std_kwd'] = map(lambda klist: [''.join(ch for ch in keyword.lower() if ch not in exclude) for keyword in klist], data['standardized_keywords'])\n",
      "    \n",
      "    #adding most popular kwrds as features\n",
      "    for elem in best_kwrds:\n",
      "        data[elem] = elem in data['std_kwd']\n",
      "\n",
      "    #adding average citation count of references\n",
      "    data['av_cit_of_references'] = map(average_cit_count, data['citations'])    \n",
      "\n",
      "    \n",
      "    #adding citation count after 5 years \n",
      "    data['cit_count_5'] = map(lambda x: int(recid_citation[x]['cit_5']) if x in recid_citation else 0, data['recid'])\n",
      "    \n",
      "    #adding citation category as outcome \n",
      "    data['cit_tot_cat'] = map(lambda x: inspire_citation_category(float(x)), data['cit_count'])\n",
      "    \n",
      "    #adding 5-year-citation category as outcome \n",
      "    data['cit_5_cat'] = map(lambda x: inspire_citation_category(float(x)), data['cit_count_5'])\n",
      "        \n",
      "    return data\n",
      "\n",
      "data = enhance_data(raw_data)\n",
      "print data.columns"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Index([u'abstract', u'citations', u'free_keywords', u'recid', u'references', u'standardized_keywords', u'title', u'tot_auth_count', u'tot_auth_list', u'cit_count', u'free_kwd_count', u'std_kwd_count', u'ref_count', u'title_len', u'abstract_len', u'date', u'year', u'std_kwd', u'numerical calculations', u'talk', u'numerical calculations interpretation of experiments', u'bibliography', u'experimental results', u'supersymmetry', u'electron positron annihilation', u'numerical calculations monte carlo', u'quantum chromodynamics', u'thesis', u'field theory scalar', u'string model', u'dimension 2', u'perturbation theory higherorder', u'cp violation', u'critical phenomena', u'scattering heavy ion', u'lattice field theory', u'electron positron colliding beams', u'gauge field theory yangmills', u'cern lhc coll', u'field theory conformal', u'field equations solution', u'gravitation', u'supergravity', u'neutrino oscillation', u'annihilation electron positron', u'hamiltonian formalism', u'dimension 3', u'cosmological model', u'effective lagrangian', u'feynman graph', u'av_cit_of_references', u'cit_count_5', u'cit_tot_cat', u'cit_5_cat'], dtype='object')\n"
       ]
      }
     ],
     "prompt_number": 69
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Even though it is a little problematic to use a random k-fold cross validation if the feature 'av_cit_of_references' is included, we will start with that for simplicity and correct that in the future."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# cross validation: dividing data set into train and test data\n",
      "\n",
      "# function to evaluate the model:\n",
      "def score(clf, Xdata, ydata):\n",
      "    # use k-fold to divide train and validation\n",
      "    kf = KFold(n, n_folds=5)\n",
      "    # will get 5 scores\n",
      "    scores = []\n",
      "    for train, test in kf:\n",
      "        clf.fit(Xdata[train], ydata[train])\n",
      "        # use sclearn score:\n",
      "        scores.append(clf.score(Xdata[test], ydata[test]))\n",
      "    return sum(scores)/len(scores)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 70
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# choose keys for machine learning\n",
      "n = len(data)\n",
      "feats = ['year', 'tot_auth_count', 'ref_count','title_len', 'abstract_len',\n",
      "         'av_cit_of_references'] + best_kwrds\n",
      "#print feats\n",
      "\n",
      "X = np.array(data.get(feats))\n",
      "y = np.array(data.cit_tot_cat)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 63
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print score(RandomForestClassifier(), X, y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.716088342103\n"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# choose keys for machine learning\n",
      "n = len(data)\n",
      "feats = ['year', 'tot_auth_count', 'ref_count','title_len', 'abstract_len', \n",
      "         'av_cit_of_references'] + best_kwrds\n",
      "#print feats\n",
      "\n",
      "X2 = np.array(data.get(feats))\n",
      "y2 = np.array(data.cit_5_cat)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 65
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print score(RandomForestClassifier(), X2, y2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.751552656933\n"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This simple model works not too bad, to be modified and we'll get better."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "  "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}